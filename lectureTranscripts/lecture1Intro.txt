Welcome everyone to DATA 442.  I'm very excited to offer this course, and thrilled to see all of you here. 

A few notes on the course - first, all lectures (and these videos) will be up on the ICSS website for you to view.  If for any reason you aren't able to attend class, you will be able to check out the lecture within a few hours of recording.  The URL is icss.wm.edu/data442/, and you can also find the URL in your syllabus.  

Second, you will also find a number of resources on that website, including course policies and other links.  This includes links for the submission of your assignments, course policies, and details on office hours.  We also provide you with links to the many courses that this course is built on top of, and encourage you to explore them to learn even more!

Third, all assignments in this course are automatically graded by - you guessed it - AI.  Given that robots aren't perfect, and some of you may dislike being judged by a machine intelligence, you can of course request your grades be reviewed by a human.  Please note that if you make such a request, this can result in your grade going up *or* down, as it will be completely regraded, so be cautious!

====== SLIDE 2
With that, let's get started.  This course is heavily focused on computer vision - how we train algorithms to interpret data collected by sensors that is represented in a graphical form.  There has been an enormous increase in the amount of graphical data - from smart phones, to cameras, to satellites, we are collecting tremendous amounts of graphical data being produced every minute.  We can look at the hard numbers to see just how incredible the scale of graphical data production is - according to Sandvine, in 2019 we saw over 60% of the data downloaded over the internet was for video streaming; this contrasts to the second-place contender, websites, in at 13.1%. 

Think about what that includes - every picture on your phone, the video of your dog rolling in the mud, and the latest episode of 'The Office' that you chose to stream.  This poses an enormous challenge for algorithms that seek to understand this information - sorting through what is important and what is not from thousands of satellite images after a natural disaster, identifying illegal or threatening activities, 
==SLIDE
or enabling you to search for a "ridiculous coffee maker" on google image search.

Making the challenge of image data even bigger is the fact that computers - as they have been coded over the last 100 or so years - are not well equipped to analyze it.  Instead, nearly all of the algorithms we have derived have focused on analyzing discrete "units of observation" - think rows in a spreadsheet.  Further, the human mind has a hard time analyzing imagery at scale - when every frame of this massive amount of data could contain key information, and the differences between those slides important, we are simply ill-equipped to capture the changes. In fact, if we wanted a human being to watch every minute of footage uploaded to youtube in real time, it would require nearly 18,000 humans!  This has critical implications for industry - if an advertiser is willing to pay to guarantee they won't have an ad put alongside - for example - violent footage, we need to have an economically viable way to do that.

===SLIDE
Because of the scope of the challenge, computer vision has tremendous overlap with a wide range of disciplines.  When we think about computer vision, we're really thinking about the algorithms required to interpret image content - for example, if a picture is a dog or a cat.  This is closely related to both image processing (i.e., engineering to make satellite imagery usable by correcting for the altitude of a sensor) and machine vision (i.e., teaching a robot how to interpret signals to minimize the chance it will fall over).  

If you consider the types of knowledge needed to replicate the process of human data intake through our eyes, processing and transport to our brains, and then cognition of that data - it's really quite a lot!  We draw on fields like neuorobiology to understand how the human eye works and sends information - much of what you will learn in this class seeks to replicate biophysical processes.  We rely on math, computer science, and related disciplines to implement the algorithms required.  And, we rely on a range of physical sciences to help design and interpret the information from sensors.  Increasingly, we also rely on physical engineering to help build new hardware that allow us to implement algorithms for computer vision more effeciently than is possible today.

===SLIDE
To give you a little background on where your instructional team is coming from, we (myself and the TAs) are based out of the Initiative for Computational Societal and Security Research at William & Mary (the ICSS).  In this group, we focus on a range of projects related to computer vision, ranging from predicting road quality from satellites, to school test scores from street view imagery, to computer vision to parse archival documents.

===SLIDE
This course is interrelated with a number of other courses offered at William & Mary, and assumes some baseline knowledge from other courses.  First and foremost, we assume a baseline proficiency in Python; this can be achieved by completing DATA 141 or CSCI 140.  If you are sitting in this room and uncomfortable with basic python, you may want to reconsider!  DATA 310 provides baseline knowledge on the ways that machine learning algorithms work, and you should be comfortable with concepts like optimization, hyperparameter optimization, and neural network backpropogation.  Finally, this course provides a very heavy focus on computer vision specifically, with a big dose of deep learning (as one of the most popular CV techniques today).

===SLIDE
One of the first topics we'll be exploring is the history of computer vision, to give you a breadth of background on how we got to where we are today.  We'll start 2 billion years ago, with the Protists; creatures that continue to evolve on earth today.  The image you see here is Eythropsidium (we'll call her 'E') - and about 50 microns in length.  It has what we might consider one of the first examples of an "eye", though we don't know exactly when it evolved; further, 'E' has no brain, and so little is known about how the information captured by this "eye" is actually used.  

===SLIDE
Approximately 635 million years ago, we believe Jellies and Sponges began evolving structures similar to eyes, though with extreme limits in what wavelengths they could capture, and relatively little known about how the information taken in was processed.  

===SLIDE
Starting approximately 543 million years ago, we saw a massive explosion of species with eyes, with the trilobite Olenellus fowleri being the first we can identify in the fossil record.  While it likely was not the first, it is the first we currently have physical evidence of.  This led to a massive rush of evolution, with the fundamental nature of life changing to adapt to the new capabilities that vision brought on.  I encourage you to read more in the article at the bottom of this slide (by professor Dr. Schwab at UC Davis's Department of Ophtalmology and Vision Science), but it is sufficient to say that we now observe vision as a core element of nearly all capabilities of intellegent creatures - from manipulation, to transportation, to mating, nearly all species (including humans) primarily rely on visual data to interact with the world.

===SLIDE
In many ways, computers have been living in a way similar to organic animals prior to vision being evolved; similar to organic animals evolution of the physical sensors to take in light (eyes), and the ways to process and understand the data generated (neurons, the brain), so too are innovations in computational capabilities advancing computers towards a new era of capability.  The physical "eyes" of computers are largely sensors that we have created to capture light artificially; we can trace these innovations all the way back to Abū ʿAlī al-Ḥasan ibn al-Ḥasan ibn al-Haytham, known in the west as "Alhazen".  He created a pinhole camera ("Camera Obscura"), where light would enter into a small area, and then be projected onto a screen inside of a container.  In 1839, this was married with the idea of film when Louis Daguerre used silver iodide-based solutions that would produce an image when exposed to light.  The evolution of film continued rapidly through 1885, when George Eastman began the production of paper film and the "Kodak Camera", which laid the groundwork for much of the mass photography we have today.

===SLIDE
As the evolution of sensors continued, so too did our understanding of how visual data was interpreted by the mind.  In 1959, Hubel and Wiesel conducted studies in which the electrical responses of cats minds (i.e., when neurons were firing) were measured as they were shown different types of shapes on a screen.  Much of computer vision was inspired by this work - it was one of our first opportunities to understand how mammals processed data which came in through the eye.  The key thing they learned was that in the visual cortex of the cat's brain simple structures - relatively simple groups of neurons - directly responded to stimulus, and especially simple stimulus like changing the orientation of edges in an image.  These simple structures then passed their findings (electrical impulses) on to more complex structures, allowing for an integration of information and conceptualization of visual inputs.  This general approach is what forms the basis of the deep learning strategies you'll be learning about in this course.

===SLIDE
Alongside developments in physical sensors and our understanding of human intuition, the computer science community was also evolving to be able to represent the world around us in computer code.  In 1963, Larry Roberts began this process by conceptualizing features in the world as "blocks", which are defined by feature points delineating the difference between objects.  This provided the fundamental logic required to enable a computer to save, and then reconstruct, three dimensional shapes.  However, this method proved limited in that not all things can be communicated with blocks!  Thus, in the 1970s a push occurred to try and improve our ability to represent objects in the "real world", with scholars such as Brooks, Binford, Fichler and Elschlager leading the charge.  The big idea these works contributed to was the idea that complicated geometrical objects can be represented by a collection of simple primitives.  David Lowe built on this idea in the 1980s, seeking to identify objects based on outlines (in other words, edges - sound familiar?).  These few cases formed the basis of the field of object recognition, but were heavily limited in their capabilities.

===SLIDE
Another paradigm of imagery recognition that was being explored in parallel to computational definitions of objects was the computational *identification* of objects.  While it can be very hard to figure out what an object is (i.e., a ball, a razor, an umbrella), it - hypothetically - should be easier to identify what objects are in a scene.  Initial attempts at segmentation began as early as 1968, with Muerle and Allen exploring strategies for defining areas within images as belonging to the same "object"; this was built on extensively by the remote sensing community who were interested in using segmentation to take advantage of another new technology emerging, satellite data.  Today, object segmentation is a common first-step in object recognition, where first unique objects in images are identified, and second the appropriate labels are applied to those objects.

===SLIDE
An enormous leap was made in 2001, when Viola and Jones showed it was possible to localize where faces were in images in near-real-time, with relatively little processing power, using an adaboost-based algorithm.  This work was not only important for it's illustration of the capability of machine learning for imagery recognition, but boosted computer vision into the forefront of industrial innovation - today, nearly every camera shows the bounding box of faces!  Simultaneously, research was probing into how objects could be identified even though they may regularly change - for example, a sunflower may appear different due to a change in petals; a logo may appear differently if it is on a curved object; or, generally, objects may appear different because of the angle a picture was taken, shadows, or other challenges.  In 1999, Lowe came to the realization that even under these variable contexts, there were some features that remained "Invariant" - i.e., they would not change irrespective of the angle they were viewed at.  Thus, he reduced the challenge of object recognition down into a first stage: seeking to identify what these invariant features were. 

===SLIDE
Starting in 2006, many of the algorithms engaging with object detection, segmentation, and recognition came together in the first of a series of large-scale challenges posed by the computer science community.  Dr. Mark Everingham of the University of Leeds compiled a large dataset (PASCAL), containing consumer photographs from Flickr which are labeled according to the objects they contain (i.e., sheep, dog, potted plant).  This provided the first large-scale dataset of images, with 500,000 labeled data points which could be used to establish the accuracy of computer vision algorithms in real-world settings.  From 2006 until 2012, an annual competition was held in which training data was provided to different groups to test, and then a testing set was used to ascertain the accuracy of their findings.  This competition was predicated on the 20 classes of objects PASCAL identified - and naturally led to a bigger question: what about everything else?

===SLIDE
Enter Imagenet, which was a collaboration across numerous groups of computer vision researchers and is currently hosted by the Stanford Vision Lab.  Imagenet is still being improved today, and represents nearly 22,000 categories of objects, with over 14 million images.  This posed an all-new challenge: how accurately could computer vision algorithms predict from the entire world of data?  This was a critical breakthrough for the computer vision community: not only was there a broad dataset available to test algorithms, but a dataset large enough to mitigate the challenge of overfitting for imagery data.  Because imagery data is highly complex - both in terms of the total amount of data, but also the types of features that might be detected - it is particularly prone to challenges associated with overfitting, and it is very difficult to determine those challenges with small datasets.  Imagenet - the product of a massive effort built on top of Amazon's mechanical turk - was a key step forward in overcoming these challenges.

===SLIDE
ImageNet spurred a number of competitions, as well as extensive development of different computer vision architectures for the classification, localization, and object detection of information in images.  One of the best summaries of these was prepared by Olga Russakovsky in 2014, which showed the improvemennt of these models from 2012 through 2014, as well as the challenges that still remained at that time.  This slide shows the error of algorithms in identifying the class of an entire image - i.e., if only the class of the item in the image needs to be established.  As this shows, over a two year period from 2012 to 2014 our ability to discern between classes increased dramatically, from approximately 83% accuracy to 93% accuracy.

==SLIDE
Here, you see the ability of models to identify the location of a specific object within an image.  As you may notice, the cases where your own eye may have an easier time discering the object in question were also easier for the algorithms in question.  Similar to the image classificaiton challenge, accuracy for localization improved by approximately 10% over this two-year period, but still remained around 75%.  While I do not show the results (I encourage you to go read the paper at the bottom of this slide!), the accuracy for the most challenging estimate - object detection across the entire image - resulted in a precision of around 37% (indicating many objects were still being missed, missclassified, or erroneously identified).

===SLIDE
There are two big moments in the evolution of ImageNet modeling efforts.  First, in 2012 the AlexNet (previously SuperVision) provided an enormous gain in accuracy over past models - nearly a 10% improvement over anything that had come before.  This network approach was predicated on a "Convolutional Neural Network", or CNN - the style of network that will form the basis of nearly everything we do this semester.

The second big moment came in 2015, He et al. introduced a new algorithm - ResNet - which for the first time was able to achieve an image classificaiton error rate lower than a human classifier.  In this study, ResNet classified nearly 97% of images correctly, as contrasted to the approximately 95% classified correctly by a Human in Russakovsky's earlier work.  The big innovation in He's work was depth - the number of layers used in the network, which gave birth to the term "Deep Learning."

Of note - the Human in Russakovsky's paper was one poor, lonely Ph.D. student at Stanford.  Aren't you glad you aren't at Stanford?

===SLIDE
Now, we're going to shift our focus back to today, and - more importantly, DATA 442.  In this course, we will explicitly focus on a few key approaches within the field of computer vision: the processes of image classificaiton, image segmentation, object detection, and image captioning.

===SLIDE
When we talk about image classificaiton, we're talking about classifying the entire image - "Does this image contain a Dog" would be a common example of a classification challenge.

===SLIDE
When we talk about Image Segmentation, we're focusing on breaking the image into it's constituent components - what pixels represent the same objects in the image?

===SLIDE
Object Detection is an even bigger challenge - can we localize where objects are, and label them?

===SLIDE
Finally, Image captioning focuses on the intersection between natural language processing and image recognition - writing sentences which describe images based on their contents.

===SLIDE
In this course, we'll be heavily focusing on Convolutional Neural Networks - not just for fun, but because since 2012 they have reigned supreme over nearly every class of image recognition challenge.  Fundamentally, the architecture of CNNs has not changed tremendously since their inception - AlexNet and ResNet both still share a number of similarities with the initial model you see here (a model designed to auotmatically read checks!).  Core concepts - how convolutions are done, feature maps, subsampling, pooling, and fully connected layers are all still present today - and we'll be doing a deep dive into each of these.  